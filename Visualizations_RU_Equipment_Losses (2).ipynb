{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block A0 (robust) — Clean unmount + fresh Google Drive mount\n",
        "# Why:\n",
        "#   - Fixes \"Mountpoint must not already contain files\" errors.\n",
        "#   - Ensures nothing wrote to /content/drive before mounting.\n",
        "# Run this as the FIRST cell in the notebook.\n",
        "# ============================================================\n",
        "\n",
        "import os, shutil, subprocess, time\n",
        "\n",
        "MOUNTPOINT = \"/content/drive\"\n",
        "\n",
        "def _safe_unmount(mp=MOUNTPOINT):\n",
        "    # 1) Try Colab's unmount (if previously mounted)\n",
        "    try:\n",
        "        from google.colab import drive as _d\n",
        "        _d.flush_and_unmount()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Try system-level unmounts (ignore failures)\n",
        "    for cmd in ([\"fusermount\", \"-u\", mp], [\"umount\", mp]):\n",
        "        try:\n",
        "            subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 3) If the directory exists and is NOT a mount, remove it\n",
        "    try:\n",
        "        if os.path.isdir(mp) and not os.path.ismount(mp):\n",
        "            shutil.rmtree(mp)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 4) Recreate empty mountpoint\n",
        "    os.makedirs(mp, exist_ok=True)\n",
        "\n",
        "# Clean up any stale state, then mount\n",
        "_safe_unmount()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(MOUNTPOINT, force_remount=True)\n",
        "\n",
        "# Sanity check\n",
        "assert os.path.isdir(\"/content/drive/MyDrive\"), \"Drive not mounted at /content/drive/MyDrive\"\n",
        "print(\"✅ Google Drive mounted and ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYogqVi_yKyH",
        "outputId": "e3ed3974-af3f-40c6-ac7f-502c4227cbde"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted and ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_0Us0pTICSEX"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# A1 — Paths + latest silver resolver + load df (WarSpotting silver)\n",
        "# ============================================================\n",
        "import os, re\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/osint_mvp\"  # no trailing slash\n",
        "\n",
        "paths = {\n",
        "    \"staged_dir\": f\"{DRIVE_ROOT}/staged\",\n",
        "    \"out_dir\":    f\"{DRIVE_ROOT}/out\",\n",
        "    \"plots\":      f\"{DRIVE_ROOT}/plots\",\n",
        "}\n",
        "\n",
        "os.makedirs(paths[\"plots\"], exist_ok=True)\n",
        "\n",
        "# Find *latest* warspotting_norm_YYYY-MM-DD.csv in staged/\n",
        "silver_candidates = sorted(glob(f\"{paths['staged_dir']}/warspotting_norm_*.csv\"))\n",
        "if not silver_candidates:\n",
        "    raise FileNotFoundError(\n",
        "        f\"No silver files found at {paths['staged_dir']}/warspotting_norm_*.csv.\\n\"\n",
        "        \"Run the ETL notebook first.\"\n",
        "    )\n",
        "\n",
        "silver_latest = silver_candidates[-1]\n",
        "print(\"Using silver file:\", silver_latest)\n",
        "\n",
        "# Load per-loss normalized data (this is what analytics should use)\n",
        "df = pd.read_csv(silver_latest, parse_dates=[\"date\"])\n",
        "\n",
        "# Derive a run label from filename for saving plots consistently\n",
        "m = re.search(r\"warspotting_norm_(\\d{4}-\\d{2}-\\d{2})\\.csv$\", os.path.basename(silver_latest))\n",
        "RUN_LABEL = m.group(1) if m else \"latest\"\n",
        "\n",
        "print(f\"Loaded {len(df)} rows. RUN_LABEL={RUN_LABEL}\")\n",
        "display(df.sample(min(3, len(df))))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block S1 — Daily spike detection (per class)\n",
        "# Purpose:\n",
        "#   - Quantify/visualize days with unusually high losses (e.g., ≥10).\n",
        "#   - Save:\n",
        "#       1) daily line with threshold overlay\n",
        "#       2) monthly bar of \"spike days\"\n",
        "# Config:\n",
        "#   - target_classes: which platform classes to analyze\n",
        "#   - SPIKE_THRESHOLD: \"large assault\" proxy (tune as needed)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Config ---\n",
        "target_classes   = [\"MBT\", \"IFV\"]   # add more if you like\n",
        "SPIKE_THRESHOLD  = 10               # try 8–12 to test sensitivity\n",
        "\n",
        "# Ensure date is datetime (if not already)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
        "\n",
        "def ensure_plots_dir():\n",
        "    os.makedirs(paths[\"plots\"], exist_ok=True)\n",
        "\n",
        "def daily_counts_for_class(df_silver, cls):\n",
        "    d = df_silver[df_silver[\"platform_class\"] == cls].copy()\n",
        "    if d.empty:\n",
        "        return pd.DataFrame(columns=[\"date\",\"n_daily\"])\n",
        "    g = (d.groupby(\"date\", as_index=False)[\"count\"].sum()\n",
        "           .rename(columns={\"count\":\"n_daily\"}))\n",
        "    g = g.sort_values(\"date\")\n",
        "    return g\n",
        "\n",
        "def plot_spikes(df_daily, cls, threshold=SPIKE_THRESHOLD):\n",
        "    if df_daily.empty:\n",
        "        print(f\"[S1] No data for {cls}\")\n",
        "        return\n",
        "    ensure_plots_dir()\n",
        "\n",
        "    # 1) Daily line with threshold overlay\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(df_daily[\"date\"], df_daily[\"n_daily\"], label=f\"{cls} daily\")\n",
        "    plt.axhline(threshold, linestyle=\"--\", label=f\"spike≥{threshold}\")\n",
        "    plt.title(f\"Daily {cls} losses with spike threshold ≥ {threshold}\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Count per day\"); plt.legend(); plt.tight_layout()\n",
        "    out1 = os.path.join(paths[\"plots\"], f\"spikes_daily_{cls}_{RUN_ID}.png\")\n",
        "    plt.savefig(out1, dpi=160); plt.close()\n",
        "    print(\"Saved:\", out1)\n",
        "\n",
        "    # 2) Monthly bar of spike-day counts\n",
        "    m = pd.DataFrame(df_daily)\n",
        "    m[\"date\"] = pd.to_datetime(m[\"date\"])\n",
        "    m[\"is_spike\"] = m[\"n_daily\"] >= threshold\n",
        "    monthly = (m.groupby(m[\"date\"].dt.to_period(\"M\"))[\"is_spike\"]\n",
        "                 .sum().rename(\"spike_days\").reset_index())\n",
        "    monthly[\"month\"] = monthly[\"date\"].astype(str)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.bar(monthly[\"month\"], monthly[\"spike_days\"])\n",
        "    plt.title(f\"Spike-day count by month — {cls} (≥{threshold})\")\n",
        "    plt.xlabel(\"Month\"); plt.ylabel(\"# of spike days\"); plt.tight_layout()\n",
        "    out2 = os.path.join(paths[\"plots\"], f\"spikes_monthly_{cls}_{RUN_ID}.png\")\n",
        "    plt.savefig(out2, dpi=160); plt.close()\n",
        "    print(\"Saved:\", out2)\n",
        "\n",
        "# ---- Run for each class ----\n",
        "for cls in target_classes:\n",
        "    s1_daily = daily_counts_for_class(df, cls)\n",
        "    plot_spikes(s1_daily, cls, threshold=SPIKE_THRESHOLD)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9mMn9k8MkM_",
        "outputId": "db43df13-4ca9-448c-a3e5-83386c087519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/osint_mvp/plots/spikes_daily_MBT_2025-09-08.png\n",
            "Saved: /content/drive/MyDrive/osint_mvp/plots/spikes_monthly_MBT_2025-09-08.png\n",
            "Saved: /content/drive/MyDrive/osint_mvp/plots/spikes_daily_IFV_2025-09-08.png\n",
            "Saved: /content/drive/MyDrive/osint_mvp/plots/spikes_monthly_IFV_2025-09-08.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block S2 — Rolling average losses over time\n",
        "# Purpose:\n",
        "#   - Show tempo shift by smoothing daily counts (e.g., 7/14-day).\n",
        "#   - Save PNG per class.\n",
        "# Notes:\n",
        "#   - Rolling window on calendar days; handles sparse days by reindexing.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "ROLLING_WINDOWS = [7, 14]      # tweak as you like\n",
        "classes_for_ra  = [\"MBT\", \"IFV\"]\n",
        "\n",
        "def daily_with_full_range(df_silver, cls):\n",
        "    d = df_silver[df_silver[\"platform_class\"] == cls].copy()\n",
        "    if d.empty:\n",
        "        return pd.DataFrame(columns=[\"date\",\"n_daily\"])\n",
        "    d[\"date\"] = pd.to_datetime(d[\"date\"])\n",
        "    g = (d.groupby(\"date\", as_index=False)[\"count\"].sum()\n",
        "           .rename(columns={\"count\":\"n_daily\"}))\n",
        "    # reindex to full continuous date range (fill missing days with 0)\n",
        "    idx = pd.date_range(g[\"date\"].min(), g[\"date\"].max(), freq=\"D\")\n",
        "    g = g.set_index(\"date\").reindex(idx).fillna(0.0).rename_axis(\"date\").reset_index()\n",
        "    g[\"n_daily\"] = g[\"n_daily\"].astype(int)\n",
        "    return g\n",
        "\n",
        "def plot_rolling_average(df_daily, cls, windows=ROLLING_WINDOWS):\n",
        "    if df_daily.empty:\n",
        "        print(f\"[S2] No data for {cls}\")\n",
        "        return\n",
        "    ensure_plots_dir()\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    # raw line (thin)\n",
        "    plt.plot(df_daily[\"date\"], df_daily[\"n_daily\"], linewidth=1, label=\"daily\")\n",
        "\n",
        "    # rolling lines\n",
        "    for w in windows:\n",
        "        ra = df_daily[\"n_daily\"].rolling(window=w, min_periods=max(1, w//2)).mean()\n",
        "        plt.plot(df_daily[\"date\"], ra, linewidth=2, label=f\"{w}-day avg\")\n",
        "\n",
        "    plt.title(f\"Rolling average losses — {cls}\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Count per day\"); plt.legend(); plt.tight_layout()\n",
        "    out = os.path.join(paths[\"plots\"], f\"rolling_avg_{cls}_{RUN_ID}.png\")\n",
        "    plt.savefig(out, dpi=160); plt.close()\n",
        "    print(\"Saved:\", out)\n",
        "\n",
        "for cls in classes_for_ra:\n",
        "    ra_daily = daily_with_full_range(df, cls)\n",
        "    plot_rolling_average(ra_daily, cls, windows=ROLLING_WINDOWS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGMdDo_CMmu8",
        "outputId": "4f958bc8-567e-4bcd-d907-6882462b19c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/osint_mvp/plots/rolling_avg_MBT_2025-09-08.png\n",
            "Saved: /content/drive/MyDrive/osint_mvp/plots/rolling_avg_IFV_2025-09-08.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block S3 — Histogram of per-unit per-day losses\n",
        "# Purpose:\n",
        "#   - Use unit/day loss counts as a proxy for assault size/clustering.\n",
        "#   - Shows if the tail (big per-unit/day events) is shrinking.\n",
        "#   - Saves a PNG per class.\n",
        "# Caveats:\n",
        "#   - Relies on `unit_canonical` (or `unit_text`) being present for some rows.\n",
        "#   - You can collapse to 'unit_text' if canonical is mostly null.\n",
        "# ============================================================\n",
        "\n",
        "# Choose which column to use for unit ID\n",
        "UNIT_COL = \"unit_canonical\"     # fallback to \"unit_text\" if needed\n",
        "classes_for_hist = [\"MBT\", \"IFV\"]\n",
        "\n",
        "def unit_day_counts(df_silver, cls, unit_col=UNIT_COL):\n",
        "    d = df_silver[(df_silver[\"platform_class\"] == cls)].copy()\n",
        "    d = d[d[unit_col].notna()]  # keep rows with unit info\n",
        "    if d.empty:\n",
        "        return pd.DataFrame(columns=[\"unit\", \"date\", \"n\"])\n",
        "    d[\"date\"] = pd.to_datetime(d[\"date\"]).dt.date\n",
        "    g = (d.groupby([unit_col, \"date\"], as_index=False)[\"count\"]\n",
        "           .sum().rename(columns={unit_col:\"unit\", \"count\":\"n\"}))\n",
        "    return g\n",
        "\n",
        "def plot_unit_day_hist(g, cls):\n",
        "    if g.empty:\n",
        "        print(f\"[S3] No unit/day data for {cls}\")\n",
        "        return\n",
        "    ensure_plots_dir()\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.hist(g[\"n\"], bins=range(1, g[\"n\"].max()+2), align=\"left\")\n",
        "    plt.title(f\"Per-unit per-day loss distribution — {cls}\")\n",
        "    plt.xlabel(\"Losses per unit on a single day\"); plt.ylabel(\"Frequency\")\n",
        "    plt.tight_layout()\n",
        "    out = os.path.join(paths[\"plots\"], f\"hist_unit_day_{cls}_{RUN_ID}.png\")\n",
        "    plt.savefig(out, dpi=160); plt.close()\n",
        "    print(\"Saved:\", out)\n",
        "\n",
        "for cls in classes_for_hist:\n",
        "    g = unit_day_counts(df, cls, unit_col=UNIT_COL)\n",
        "    plot_unit_day_hist(g, cls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCxdQV8IMxON",
        "outputId": "bd7d6ff5-2f45-4665-ecd4-fb8a925d122c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/osint_mvp/plots/hist_unit_day_MBT_2025-09-08.png\n",
            "Saved: /content/drive/MyDrive/osint_mvp/plots/hist_unit_day_IFV_2025-09-08.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block G3 — Early vs Late density on a real basemap (shared scale)\n",
        "# Purpose:\n",
        "#   - Compare spatial concentration across two time windows fairly.\n",
        "#   - Same extent, same gridsize, shared Log color scale.\n",
        "# Outputs:\n",
        "#   - plots/geo_hexbin_early_vs_late_<class>_<RUN_ID>.png\n",
        "# Reqs:\n",
        "#   - pip install geopandas contextily shapely pyproj (once per runtime)\n",
        "# ============================================================\n",
        "\n",
        "# 1) Install (safe to re-run)\n",
        "!pip -q install geopandas contextily shapely pyproj >/dev/null\n",
        "\n",
        "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, box\n",
        "import contextily as cx\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "# --- Params you can tweak ---\n",
        "target_class = \"SPG\"  # e.g. \"MBT\", \"IFV\", \"MLRS\", \"SPG\", \"Truck\"\n",
        "\n",
        "early_start = \"2025-06-01\"\n",
        "early_end   = \"2025-07-07\"\n",
        "\n",
        "late_start  = \"2025-07-08\"\n",
        "late_end    = \"2025-09-07\"\n",
        "\n",
        "gridsize    = 60      # density resolution (bigger = coarser)\n",
        "padding_m   = 40_000  # padding around combined extent in meters\n",
        "plots_dir   = paths[\"plots\"]  # from your earlier setup\n",
        "os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "# --- Helpers ---\n",
        "def to_gdf_wgs84(df_):\n",
        "    d = df_.copy()\n",
        "    d = d[d[\"lat\"].notna() & d[\"lon\"].notna()]\n",
        "    if d.empty:\n",
        "        return gpd.GeoDataFrame(d, geometry=[], crs=\"EPSG:4326\")\n",
        "    return gpd.GeoDataFrame(d, geometry=gpd.points_from_xy(d[\"lon\"], d[\"lat\"]), crs=\"EPSG:4326\")\n",
        "\n",
        "def filter_window(df_, start, end, cls):\n",
        "    mask_d = (pd.to_datetime(df_[\"date\"]) >= pd.to_datetime(start)) & \\\n",
        "             (pd.to_datetime(df_[\"date\"]) <= pd.to_datetime(end))\n",
        "    mask_c = (df_[\"platform_class\"] == cls)\n",
        "    return df_.loc[mask_d & mask_c].copy()\n",
        "\n",
        "# --- Build GeoDataFrames for the two windows (WGS84) ---\n",
        "df_early = filter_window(df, early_start, early_end, target_class)\n",
        "df_late  = filter_window(df, late_start,  late_end,  target_class)\n",
        "\n",
        "gdf_e_wgs = to_gdf_wgs84(df_early)\n",
        "gdf_l_wgs = to_gdf_wgs84(df_late)\n",
        "\n",
        "# Limit roughly to Ukraine to avoid stray points; adjust if you like\n",
        "UKR_BBOX_WGS84 = box(22.0, 44.0, 41.5, 52.5)\n",
        "gdf_e_wgs = gdf_e_wgs[gdf_e_wgs.geometry.within(UKR_BBOX_WGS84)]\n",
        "gdf_l_wgs = gdf_l_wgs[gdf_l_wgs.geometry.within(UKR_BBOX_WGS84)]\n",
        "\n",
        "if gdf_e_wgs.empty and gdf_l_wgs.empty:\n",
        "    print(f\"[G3] No geo-tagged points for class '{target_class}' in either window.\")\n",
        "else:\n",
        "    # Project to Web Mercator for tiles\n",
        "    gdf_e = gdf_e_wgs.to_crs(epsg=3857)\n",
        "    gdf_l = gdf_l_wgs.to_crs(epsg=3857)\n",
        "\n",
        "    # Compute a combined plot extent (same zoom for both panels)\n",
        "    xs = np.concatenate([gdf_e.geometry.x.values if not gdf_e.empty else np.array([]),\n",
        "                         gdf_l.geometry.x.values if not gdf_l.empty else np.array([])])\n",
        "    ys = np.concatenate([gdf_e.geometry.y.values if not gdf_e.empty else np.array([]),\n",
        "                         gdf_l.geometry.y.values if not gdf_l.empty else np.array([])])\n",
        "\n",
        "    if len(xs) == 0:  # one window might be empty, the other not; fit to the non-empty one\n",
        "        xs = gdf_e.geometry.x.values if not gdf_e.empty else gdf_l.geometry.x.values\n",
        "        ys = gdf_e.geometry.y.values if not gdf_e.empty else gdf_l.geometry.y.values\n",
        "\n",
        "    x_min, x_max = xs.min()-padding_m, xs.max()+padding_m\n",
        "    y_min, y_max = ys.min()-padding_m, ys.max()+padding_m\n",
        "    extent = (x_min, x_max, y_min, y_max)\n",
        "\n",
        "    # Determine shared color normalization using 2D histograms (counts)\n",
        "    def counts_max(gdf_, extent, gridsize):\n",
        "        if gdf_.empty:\n",
        "            return 0\n",
        "        x = gdf_.geometry.x.values\n",
        "        y = gdf_.geometry.y.values\n",
        "        H, _, _ = np.histogram2d(x, y, bins=gridsize, range=[[extent[0], extent[1]], [extent[2], extent[3]]])\n",
        "        return int(H.max())\n",
        "\n",
        "    max_e = counts_max(gdf_e, extent, gridsize)\n",
        "    max_l = counts_max(gdf_l, extent, gridsize)\n",
        "    vmax  = max(max_e, max_l, 1)  # at least 1 to keep LogNorm happy\n",
        "\n",
        "    # Plot two panels with shared Log color scale\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 6), constrained_layout=True, sharex=True, sharey=True)\n",
        "\n",
        "    def hexbin_panel(ax, gdf_, title):\n",
        "        if gdf_.empty:\n",
        "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", fontsize=12, transform=ax.transAxes)\n",
        "        else:\n",
        "            x = gdf_.geometry.x.values\n",
        "            y = gdf_.geometry.y.values\n",
        "            hb = ax.hexbin(x, y, gridsize=gridsize, mincnt=1,\n",
        "                           extent=[extent[0], extent[1], extent[2], extent[3]],\n",
        "                           norm=LogNorm(vmin=1, vmax=vmax))\n",
        "            cx.add_basemap(ax, source=cx.providers.CartoDB.Positron, crs=gdf_.crs)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlim(extent[0], extent[1])\n",
        "        ax.set_ylim(extent[2], extent[3])\n",
        "        ax.set_axis_off()\n",
        "        return hb if not gdf_.empty else None\n",
        "\n",
        "    hb1 = hexbin_panel(ax1, gdf_e, f\"Early: {early_start} → {early_end}\")\n",
        "    hb2 = hexbin_panel(ax2, gdf_l, f\"Late:  {late_start} → {late_end}\")\n",
        "\n",
        "    # Shared colorbar (if at least one panel has data)\n",
        "    mappable = hb1 or hb2\n",
        "    if mappable:\n",
        "        cbar = fig.colorbar(mappable, ax=[ax1, ax2], shrink=0.85)\n",
        "        cbar.set_label(\"Loss density (log scale)\")\n",
        "\n",
        "    fig.suptitle(f\"{target_class} losses — spatial density (Early vs Late)\", y=1.02, fontsize=14)\n",
        "\n",
        "    outpath = os.path.join(plots_dir, f\"geo_hexbin_early_vs_late_{target_class}_{RUN_ID}.png\")\n",
        "    plt.savefig(outpath, dpi=170, bbox_inches=\"tight\", pad_inches=0.05)\n",
        "    plt.close()\n",
        "    print(\"Saved:\", outpath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcUmQSvblE5A",
        "outputId": "fc4efb44-400f-4b8e-a95a-b297db7e63aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/osint_mvp/plots/geo_hexbin_early_vs_late_SPG_2025-09-08.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block G4 (fixed) — Smooth KDE heatmap on a real basemap\n",
        "# Draw basemap FIRST, then heatmap on top (with alpha + zorder)\n",
        "# ============================================================\n",
        "\n",
        "# 1) Install deps (safe to re-run)\n",
        "!pip -q install geopandas contextily shapely pyproj scikit-learn >/dev/null\n",
        "\n",
        "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import box\n",
        "import contextily as cx\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "# --- Params you can tweak ---\n",
        "target_class = \"SPG\"       # e.g., \"MBT\", \"IFV\", \"MLRS\", \"SPG\", \"Truck\"\n",
        "use_date_window = False    # set True to restrict by date\n",
        "start_date = \"2025-07-01\"\n",
        "end_date   = \"2025-08-15\"\n",
        "\n",
        "bandwidth_m = 25_000       # 15k–40k looks good\n",
        "grid_px     = 900          # width in pixels\n",
        "cmap_name   = \"inferno\"\n",
        "alpha_heat  = 0.65\n",
        "padding_m   = 40_000\n",
        "\n",
        "plots_dir = paths[\"plots\"]\n",
        "os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "# --- Quick sanity: what classes do we have? ---\n",
        "print(\"Classes present:\", sorted(df[\"platform_class\"].dropna().unique())[:20])\n",
        "\n",
        "# Filter rows (class + optional date + lat/lon present)\n",
        "df_in = df[(df[\"platform_class\"] == target_class) &\n",
        "           df[\"lat\"].notna() & df[\"lon\"].notna()].copy()\n",
        "\n",
        "if use_date_window:\n",
        "    mask = (pd.to_datetime(df_in[\"date\"]) >= pd.to_datetime(start_date)) & \\\n",
        "           (pd.to_datetime(df_in[\"date\"]) <= pd.to_datetime(end_date))\n",
        "    df_in = df_in.loc[mask]\n",
        "\n",
        "print(f\"{len(df_in)} points for class={target_class}\"\n",
        "      + (f\" in {start_date}→{end_date}\" if use_date_window else \"\"))\n",
        "\n",
        "if df_in.empty:\n",
        "    print(f\"[G4] No data for class '{target_class}' with current filters.\")\n",
        "else:\n",
        "    # GeoDataFrame WGS84 → clip to Ukraine bbox → project to Web Mercator\n",
        "    gdf_wgs = gpd.GeoDataFrame(\n",
        "        df_in, geometry=gpd.points_from_xy(df_in[\"lon\"], df_in[\"lat\"]), crs=\"EPSG:4326\"\n",
        "    )\n",
        "    UKR_BBOX_WGS84 = box(22.0, 44.0, 41.5, 52.5)\n",
        "    gdf_wgs = gdf_wgs[gdf_wgs.geometry.within(UKR_BBOX_WGS84)]\n",
        "    if gdf_wgs.empty:\n",
        "        print(f\"[G4] No points inside Ukraine bbox for class '{target_class}'.\")\n",
        "    else:\n",
        "        gdf = gdf_wgs.to_crs(epsg=3857)\n",
        "        xs = gdf.geometry.x.values\n",
        "        ys = gdf.geometry.y.values\n",
        "\n",
        "        # Extent with padding\n",
        "        x_min, x_max = xs.min() - padding_m, xs.max() + padding_m\n",
        "        y_min, y_max = ys.min() - padding_m, ys.max() + padding_m\n",
        "\n",
        "        # Grid for KDE\n",
        "        width = grid_px\n",
        "        aspect = (y_max - y_min) / (x_max - x_min)\n",
        "        height = max(1, int(width * aspect))\n",
        "        xlin = np.linspace(x_min, x_max, width)\n",
        "        ylin = np.linspace(y_min, y_max, height)\n",
        "        xx, yy = np.meshgrid(xlin, ylin)\n",
        "        grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
        "\n",
        "        # KDE\n",
        "        kde = KernelDensity(bandwidth=bandwidth_m, kernel=\"gaussian\")\n",
        "        kde.fit(np.column_stack([xs, ys]))\n",
        "        log_d = kde.score_samples(grid_points)\n",
        "        dens = np.exp(log_d).reshape(height, width)\n",
        "\n",
        "        # Robust scaling (avoid all-zero look)\n",
        "        vmax_quant = np.quantile(dens, 0.99)\n",
        "        vmax_direct = dens.max()\n",
        "        vmax = max(vmax_quant, vmax_direct * 0.7, 1e-12)\n",
        "        dens_scaled = np.clip(dens, 0, vmax) / vmax\n",
        "\n",
        "        # ---- Plot: basemap first, then heatmap on top ----\n",
        "        fig, ax = plt.subplots(figsize=(9, 7))\n",
        "\n",
        "        # Basemap (goes underneath)\n",
        "        # Use the full extent; contextily respects current CRS (EPSG:3857)\n",
        "        ax.set_xlim(x_min, x_max)\n",
        "        ax.set_ylim(y_min, y_max)\n",
        "        cx.add_basemap(ax, source=cx.providers.CartoDB.Positron, crs=\"EPSG:3857\")\n",
        "\n",
        "        # Heatmap on top (semi-transparent)\n",
        "        im = ax.imshow(\n",
        "            np.flipud(dens_scaled),\n",
        "            extent=[x_min, x_max, y_min, y_max],\n",
        "            cmap=cmap_name, alpha=alpha_heat, zorder=5\n",
        "        )\n",
        "\n",
        "        # (Optional) a few points on top as dots, to sanity-check alignment\n",
        "        if len(xs) > 0:\n",
        "            ax.scatter(xs[::max(1, len(xs)//300)], ys[::max(1, len(ys)//300)],\n",
        "                       s=6, alpha=0.6, zorder=6)\n",
        "\n",
        "        title = f\"KDE heatmap — {target_class} losses\"\n",
        "        if use_date_window:\n",
        "            title += f\" ({start_date} → {end_date})\"\n",
        "        ax.set_title(title)\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        cbar = fig.colorbar(im, ax=ax, shrink=0.85)\n",
        "        cbar.set_label(\"Relative density (scaled)\")\n",
        "\n",
        "        outpath = os.path.join(plots_dir, f\"geo_kde_heatmap_{target_class}_{RUN_ID}.png\")\n",
        "        plt.savefig(outpath, dpi=170, bbox_inches=\"tight\", pad_inches=0.05)\n",
        "        plt.close()\n",
        "        print(\"Saved:\", outpath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spTeVShlpkwK",
        "outputId": "eb488ad3-d6cc-4ded-b0b1-856fe3917006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes present: ['AD', 'Airplanes', 'Ambulances, medical vehicles', 'C2', 'Engineering', 'Helicopter', 'IFV', 'Infantry mobility vehicles', 'MBT', 'MLRS', 'Other', 'Radars, jammers', 'SPG', 'TowedArtillery', 'Truck', 'UAV']\n",
            "11 points for class=SPG\n",
            "Saved: /content/drive/MyDrive/osint_mvp/plots/geo_kde_heatmap_SPG_2025-09-08.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block G4b — Adaptive KDE heatmaps on a real basemap (multi-class)\n",
        "# Purpose:\n",
        "#   - Make \"high-signal\" heatmaps without manual parameter hunting.\n",
        "#   - One PNG per class. Uses adaptive bandwidth + robust color scaling.\n",
        "# Outputs:\n",
        "#   - plots/geo_kde_heatmap_<class>_<RUN_ID>.png\n",
        "# Reqs (once per runtime):\n",
        "#   - pip install geopandas contextily shapely pyproj scikit-learn\n",
        "# ============================================================\n",
        "\n",
        "# 1) Install deps (safe to re-run)\n",
        "!pip -q install geopandas contextily shapely pyproj scikit-learn >/dev/null\n",
        "\n",
        "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import box\n",
        "import contextily as cx\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "# --- Pick classes you want maps for ---\n",
        "target_classes = [\"MBT\", \"IFV\", \"SPG\", \"MLRS\", \"Truck\"]  # edit as you like\n",
        "\n",
        "# --- Optional: date window (set to True to restrict) ---\n",
        "USE_DATE_WINDOW = False\n",
        "DATE_START = \"2025-06-01\"\n",
        "DATE_END   = \"2025-09-07\"\n",
        "\n",
        "# --- Render controls (sane defaults) ---\n",
        "GRID_PX    = 1100        # output width (px); height auto-kept by aspect\n",
        "ALPHA_HEAT = 0.68        # heat overlay opacity\n",
        "CMAP_NAME  = \"inferno\"   # \"magma\", \"plasma\", \"viridis\" also nice\n",
        "PAD_M      = 60_000      # padding around data extent in meters\n",
        "UKR_CLIP   = True        # True = clip to coarse Ukraine bbox; set False to show all\n",
        "\n",
        "# Paths\n",
        "plots_dir = paths[\"plots\"]  # from your earlier setup\n",
        "os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "# --- Coarse Ukraine bbox (WGS84) ---\n",
        "UKR_BBOX_WGS84 = box(22.0, 44.0, 41.5, 52.5)\n",
        "\n",
        "# --- Helpers ------------------------------------------------\n",
        "def _filter_df(df_in, cls):\n",
        "    d = df_in[(df_in[\"platform_class\"] == cls) & df_in[\"lat\"].notna() & df_in[\"lon\"].notna()].copy()\n",
        "    if USE_DATE_WINDOW:\n",
        "        mask = (pd.to_datetime(d[\"date\"]) >= pd.to_datetime(DATE_START)) & \\\n",
        "               (pd.to_datetime(d[\"date\"]) <= pd.to_datetime(DATE_END))\n",
        "        d = d.loc[mask]\n",
        "    return d\n",
        "\n",
        "def _adaptive_bandwidth_m(x, y):\n",
        "    \"\"\"\n",
        "    Scott-like 2D bandwidth in meters with safeguards:\n",
        "    h = c * min(std_x, IQR_x/1.34, std_y, IQR_y/1.34) * n^(-1/6)\n",
        "    then clamp to [12.5 km, 45 km] so it looks good for slide scale.\n",
        "    \"\"\"\n",
        "    n = max(len(x), 1)\n",
        "    sx = np.std(x) if n > 1 else 0.0\n",
        "    sy = np.std(y) if n > 1 else 0.0\n",
        "    qx = np.subtract(*np.percentile(x, [75, 25])) if n > 1 else 0.0\n",
        "    qy = np.subtract(*np.percentile(y, [75, 25])) if n > 1 else 0.0\n",
        "    sig = np.nanmax([sx, sy, qx/1.34 if qx>0 else 0.0, qy/1.34 if qy>0 else 0.0])\n",
        "    if sig <= 0:\n",
        "        return 20_000  # fallback\n",
        "    h = 1.06 * sig * (n ** (-1/6))  # Scott factor for 2D\n",
        "    return float(np.clip(h, 12_500, 45_000))\n",
        "\n",
        "def _robust_scale(dens):\n",
        "    \"\"\"Scale to 99th percentile so maps aren't washed out by a few hot pixels.\"\"\"\n",
        "    if dens.size == 0:\n",
        "        return dens, 1.0\n",
        "    vmax = np.quantile(dens, 0.99)\n",
        "    if vmax <= 0:\n",
        "        return dens, 1.0\n",
        "    return np.clip(dens, 0, vmax) / vmax, vmax\n",
        "\n",
        "def _make_heatmap_for_class(df_base, cls):\n",
        "    d = _filter_df(df_base, cls)\n",
        "    print(f\"[G4b] {cls}: {len(d)} rows before bbox clip/date window\")\n",
        "    if d.empty:\n",
        "        print(f\"[G4b] Skipping {cls}: no rows.\")\n",
        "        return\n",
        "\n",
        "    # WGS84 → (optional) bbox clip → Web Mercator\n",
        "    gdf_wgs = gpd.GeoDataFrame(d, geometry=gpd.points_from_xy(d[\"lon\"], d[\"lat\"]), crs=\"EPSG:4326\")\n",
        "    if UKR_CLIP:\n",
        "        gdf_wgs = gdf_wgs[gdf_wgs.geometry.within(UKR_BBOX_WGS84)]\n",
        "        print(f\"[G4b] {cls}: {len(gdf_wgs)} rows after Ukraine bbox clip\")\n",
        "        if gdf_wgs.empty:\n",
        "            print(f\"[G4b] Skipping {cls}: empty after bbox clip. Try UKR_CLIP=False.\")\n",
        "            return\n",
        "\n",
        "    gdf = gdf_wgs.to_crs(epsg=3857)\n",
        "    xs = gdf.geometry.x.values\n",
        "    ys = gdf.geometry.y.values\n",
        "\n",
        "    # Extent + padding\n",
        "    x_min, x_max = xs.min() - PAD_M, xs.max() + PAD_M\n",
        "    y_min, y_max = ys.min() - PAD_M, ys.max() + PAD_M\n",
        "\n",
        "    # KDE grid\n",
        "    width = GRID_PX\n",
        "    aspect = (y_max - y_min) / (x_max - x_min)\n",
        "    height = max(1, int(width * aspect))\n",
        "    xlin = np.linspace(x_min, x_max, width)\n",
        "    ylin = np.linspace(y_min, y_max, height)\n",
        "    xx, yy = np.meshgrid(xlin, ylin)\n",
        "    grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
        "\n",
        "    # Adaptive bandwidth\n",
        "    bw = _adaptive_bandwidth_m(xs, ys)\n",
        "    kde = KernelDensity(bandwidth=bw, kernel=\"gaussian\")\n",
        "    kde.fit(np.column_stack([xs, ys]))\n",
        "    dens = np.exp(kde.score_samples(grid_points)).reshape(height, width)\n",
        "\n",
        "    # Robust scale to 99th pct\n",
        "    dens_scaled, vmax = _robust_scale(dens)\n",
        "\n",
        "    # --- Plot: basemap first, then heatmap + sparse dots ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "    ax.set_xlim(x_min, x_max); ax.set_ylim(y_min, y_max)\n",
        "    cx.add_basemap(ax, source=cx.providers.CartoDB.Positron, crs=\"EPSG:3857\")\n",
        "\n",
        "    im = ax.imshow(\n",
        "        np.flipud(dens_scaled),\n",
        "        extent=[x_min, x_max, y_min, y_max],\n",
        "        cmap=CMAP_NAME, alpha=ALPHA_HEAT, zorder=5\n",
        "    )\n",
        "\n",
        "    # Sparse dot overlay (sample up to ~400 points for sanity)\n",
        "    step = max(1, len(xs)//400)\n",
        "    ax.scatter(xs[::step], ys[::step], s=6, alpha=0.6, zorder=6)\n",
        "\n",
        "    title = f\"KDE heatmap — {cls}\"\n",
        "    if USE_DATE_WINDOW:\n",
        "        title += f\" ({DATE_START} → {DATE_END})\"\n",
        "    ax.set_title(title)\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    cbar = fig.colorbar(im, ax=ax, shrink=0.85)\n",
        "    cbar.set_label(\"Relative density (scaled to 99th pct)\")\n",
        "\n",
        "    out = os.path.join(plots_dir, f\"geo_kde_heatmap_{cls}_{RUN_ID}.png\")\n",
        "    plt.savefig(out, dpi=175, bbox_inches=\"tight\", pad_inches=0.05)\n",
        "    plt.close()\n",
        "    print(f\"[G4b] Saved: {out} (n={len(xs)}, bw≈{int(bw)} m)\")\n",
        "\n",
        "# ---- Run for each class ----\n",
        "for cls in target_classes:\n",
        "    _make_heatmap_for_class(df, cls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shwYgua7qNlx",
        "outputId": "fe788598-aefa-433f-87c7-015ec52e8521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[G4b] MBT: 35 rows before bbox clip/date window\n",
            "[G4b] MBT: 35 rows after Ukraine bbox clip\n",
            "[G4b] Saved: /content/drive/MyDrive/osint_mvp/plots/geo_kde_heatmap_MBT_2025-09-08.png (n=35, bw≈45000 m)\n",
            "[G4b] IFV: 95 rows before bbox clip/date window\n",
            "[G4b] IFV: 95 rows after Ukraine bbox clip\n",
            "[G4b] Saved: /content/drive/MyDrive/osint_mvp/plots/geo_kde_heatmap_IFV_2025-09-08.png (n=95, bw≈45000 m)\n",
            "[G4b] SPG: 11 rows before bbox clip/date window\n",
            "[G4b] SPG: 11 rows after Ukraine bbox clip\n",
            "[G4b] Saved: /content/drive/MyDrive/osint_mvp/plots/geo_kde_heatmap_SPG_2025-09-08.png (n=11, bw≈45000 m)\n",
            "[G4b] MLRS: 16 rows before bbox clip/date window\n",
            "[G4b] MLRS: 16 rows after Ukraine bbox clip\n",
            "[G4b] Saved: /content/drive/MyDrive/osint_mvp/plots/geo_kde_heatmap_MLRS_2025-09-08.png (n=16, bw≈45000 m)\n",
            "[G4b] Truck: 39 rows before bbox clip/date window\n",
            "[G4b] Truck: 39 rows after Ukraine bbox clip\n",
            "[G4b] Saved: /content/drive/MyDrive/osint_mvp/plots/geo_kde_heatmap_Truck_2025-09-08.png (n=39, bw≈45000 m)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C144vXE8NuMR",
        "outputId": "2c425f22-ea34-4a13-a18f-e53d6b2bc330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block S4 — Mass vehicle loss spike events over time\n",
        "# Purpose:\n",
        "#   - Count how often same-day, same-location losses > threshold\n",
        "#   - Plot per-week or per-month frequency\n",
        "# Output:\n",
        "#   - PNG bar chart in plots/ (mass_loss_spikes_<class>.png)\n",
        "# Config:\n",
        "#   - platform_class: \"MBT\", \"IFV\", etc.\n",
        "#   - threshold: what counts as a spike\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# --- Config ---\n",
        "platform_class = \"MBT\"        # Try \"IFV\" or \"SPG\" too\n",
        "spike_threshold = 6           # Same-day + location losses > 6 = spike\n",
        "group_by = \"W\"                # \"W\" = week, \"M\" = month\n",
        "plots_dir = paths[\"plots\"]\n",
        "os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "# --- Filter + clean ---\n",
        "df_spike = df[(df[\"platform_class\"] == platform_class) &\n",
        "              df[\"date\"].notna() & df[\"location\"].notna()].copy()\n",
        "df_spike[\"date\"] = pd.to_datetime(df_spike[\"date\"])\n",
        "\n",
        "# Group by (date + location) to count losses per day/location\n",
        "grp = (df_spike.groupby([\"date\", \"location\"], as_index=False)[\"count\"]\n",
        "       .sum().rename(columns={\"count\": \"n_losses\"}))\n",
        "\n",
        "# Flag \"spike events\"\n",
        "grp[\"is_spike\"] = grp[\"n_losses\"] > spike_threshold\n",
        "\n",
        "# Group by week/month: how many spike events?\n",
        "grp[\"period\"] = grp[\"date\"].dt.to_period(group_by)\n",
        "summary = grp[grp[\"is_spike\"]].groupby(\"period\").size().reset_index(name=\"n_spikes\")\n",
        "summary[\"period\"] = summary[\"period\"].astype(str)\n",
        "\n",
        "# Optional: fill missing periods\n",
        "all_periods = pd.period_range(grp[\"period\"].min(), grp[\"period\"].max(), freq=group_by)\n",
        "summary = summary.set_index(\"period\").reindex(all_periods.astype(str), fill_value=0).reset_index()\n",
        "\n",
        "# --- Plot ---\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(summary[\"index\"], summary[\"n_spikes\"])\n",
        "plt.title(f\"{platform_class} mass-loss events over time\\n(>{spike_threshold} in 1 day/region)\")\n",
        "plt.xlabel(\"Time\"); plt.ylabel(\"# of spike events\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Save ---\n",
        "outpath = os.path.join(plots_dir, f\"mass_loss_spikes_{platform_class}_{RUN_ID}.png\")\n",
        "plt.savefig(outpath, dpi=160)\n",
        "plt.close()\n",
        "print(\"Saved:\", outpath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DapVwlfLpDu4",
        "outputId": "68a0f7e6-2743-4da2-d7e6-e1e87a321191"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/osint_mvp/plots/mass_loss_spikes_MBT_2025-09-08.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block S5 — Mass loss spikes by unit (stacked by platform class)\n",
        "# Spike event: same day + same unit + platform_class with > threshold losses\n",
        "# Depends on:\n",
        "#   - df (loaded in A1, warspotting_norm_*.csv)\n",
        "#   - RUN_LABEL, paths[\"plots\"] (set in A1)\n",
        "# Outputs:\n",
        "#   - plots/mass_loss_unit_spikes_by_class_<RUN_LABEL>.png\n",
        "#   - out/spike_events_by_unit_<RUN_LABEL>.csv (optional detail export)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Config ---\n",
        "spike_threshold = 6            # >6 losses = spike\n",
        "group_by = \"W\"                 # \"W\" weekly, \"M\" monthly\n",
        "min_unit_len = 3               # ignore very short unit labels\n",
        "export_csv = True              # also save a CSV of the spike events\n",
        "\n",
        "# --- Sanity: required globals from A1 ---\n",
        "assert \"df\" in globals(), \"df is not loaded. Run A1 first.\"\n",
        "assert \"paths\" in globals() and \"plots\" in paths, \"paths['plots'] missing. Run A1 first.\"\n",
        "RUN_LABEL = globals().get(\"RUN_LABEL\", \"latest\")\n",
        "\n",
        "os.makedirs(paths[\"plots\"], exist_ok=True)\n",
        "os.makedirs(paths.get(\"out_dir\", os.path.join(paths[\"plots\"], \"..\", \"out\")), exist_ok=True)\n",
        "\n",
        "# --- Prep & unit fallback ---\n",
        "df_unit = df.copy()\n",
        "df_unit[\"date\"] = pd.to_datetime(df_unit[\"date\"], errors=\"coerce\")\n",
        "\n",
        "# create a unit column with fallback: unit_canonical -> unit_text\n",
        "if \"unit_canonical\" in df_unit.columns:\n",
        "    unit_series = df_unit[\"unit_canonical\"]\n",
        "else:\n",
        "    unit_series = pd.Series([None] * len(df_unit))\n",
        "\n",
        "if unit_series.isna().all() and \"unit_text\" in df_unit.columns:\n",
        "    unit_series = df_unit[\"unit_text\"]\n",
        "\n",
        "df_unit[\"unit\"] = unit_series.astype(\"string\").str.strip()\n",
        "df_unit.loc[df_unit[\"unit\"].isin([\"\", \"None\", \"nan\"]), \"unit\"] = pd.NA\n",
        "\n",
        "# filter rows usable for this analysis\n",
        "pre_rows = len(df_unit)\n",
        "df_unit = df_unit[\n",
        "    df_unit[\"date\"].notna()\n",
        "    & df_unit[\"platform_class\"].notna()\n",
        "    & df_unit[\"unit\"].notna()\n",
        "    & (df_unit[\"unit\"].str.len() >= min_unit_len)\n",
        "].copy()\n",
        "post_rows = len(df_unit)\n",
        "\n",
        "print(f\"[S5] Usable rows with unit info: {post_rows}/{pre_rows} \"\n",
        "      f\"({post_rows/pre_rows*100:.1f}%)\")\n",
        "\n",
        "if df_unit.empty:\n",
        "    print(\"[S5] No rows with unit/platform_class/date; nothing to plot.\")\n",
        "else:\n",
        "    # normalize count column presence\n",
        "    if \"count\" not in df_unit.columns:\n",
        "        df_unit[\"count\"] = 1\n",
        "\n",
        "    # losses per (date, unit, platform_class)\n",
        "    grp = (df_unit\n",
        "           .groupby([\"date\", \"unit\", \"platform_class\"], as_index=False)[\"count\"]\n",
        "           .sum()\n",
        "           .rename(columns={\"count\": \"n_losses\"}))\n",
        "\n",
        "    # flag spikes\n",
        "    grp[\"is_spike\"] = grp[\"n_losses\"] > spike_threshold\n",
        "\n",
        "    # period bucketing\n",
        "    if group_by not in {\"W\", \"M\"}:\n",
        "        raise ValueError(\"group_by must be 'W' (weekly) or 'M' (monthly).\")\n",
        "    grp[\"period\"] = grp[\"date\"].dt.to_period(group_by).astype(str)\n",
        "\n",
        "    spikes = grp[grp[\"is_spike\"]].copy()\n",
        "    if spikes.empty:\n",
        "        print(f\"[S5] No spike events (> {spike_threshold}) found.\")\n",
        "    else:\n",
        "        # count spikes per period & platform class\n",
        "        spike_counts = (spikes\n",
        "                        .groupby([\"period\", \"platform_class\"])\n",
        "                        .size()\n",
        "                        .reset_index(name=\"n_spikes\"))\n",
        "\n",
        "        # pivot for stacked bars; reindex to continuous periods\n",
        "        pivot = spike_counts.pivot(index=\"period\",\n",
        "                                   columns=\"platform_class\",\n",
        "                                   values=\"n_spikes\").fillna(0)\n",
        "\n",
        "        all_periods = pd.period_range(pivot.index.min(), pivot.index.max(),\n",
        "                                      freq=group_by).astype(str)\n",
        "        pivot = pivot.reindex(all_periods, fill_value=0)\n",
        "\n",
        "        # consistent column order\n",
        "        pivot = pivot[sorted(pivot.columns)]\n",
        "\n",
        "        # plot\n",
        "        fig, ax = plt.subplots(figsize=(11, 6))\n",
        "        bottom = None\n",
        "        for col in pivot.columns:\n",
        "            ax.bar(pivot.index, pivot[col], label=col, bottom=bottom)\n",
        "            bottom = (pivot[col] if bottom is None else bottom + pivot[col])\n",
        "\n",
        "        ax.set_title(f\"Same‑unit daily spike events (> {spike_threshold} losses)\\n\"\n",
        "                     f\"Stacked by platform class\")\n",
        "        ax.set_xlabel(\"Time\"); ax.set_ylabel(\"# of spike events\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "        ax.legend(ncol=3, fontsize=9)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        out_png = os.path.join(paths[\"plots\"],\n",
        "                               f\"mass_loss_unit_spikes_by_class_{RUN_LABEL}.png\")\n",
        "        plt.savefig(out_png, dpi=160)\n",
        "        plt.close()\n",
        "        print(\"Saved plot:\", out_png)\n",
        "\n",
        "        # optional: export detailed spike table for QA / slide notes\n",
        "        if export_csv:\n",
        "            out_csv = os.path.join(paths.get(\"out_dir\", f\"{os.path.dirname(paths['plots'])}/out\"),\n",
        "                                   f\"spike_events_by_unit_{RUN_LABEL}.csv\")\n",
        "            spikes.sort_values([\"date\", \"unit\", \"platform_class\", \"n_losses\"]) \\\n",
        "                  .to_csv(out_csv, index=False)\n",
        "            print(\"Saved spike events CSV:\", out_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-gODUbHpOvG",
        "outputId": "e77a45c1-8157-4124-8ab9-e78ef512ec3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S5] No spike events (> 6) found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BLOCK U1+ — Interactive per‑unit map (Folium) with basemap selector\n",
        "# What it does:\n",
        "#   - Lets you pick a unit (free-text contains match) and a basemap\n",
        "#   - Plots one marker per loss (MBT/IFV/APC/BMP by default)\n",
        "#   - Tooltip shows model, class, unit, date, nearest location\n",
        "#   - Outputs an HTML map to /plots/\n",
        "# Depends on:\n",
        "#   - df (loaded in A1), paths[\"plots\"], RUN_LABEL\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "from datetime import datetime\n",
        "\n",
        "# ---------- Config you can tweak ----------\n",
        "DEFAULT_FOCUS_CLASSES = {\"MBT\", \"IFV\", \"APC\", \"BMP\"}  # set() for all classes\n",
        "DATE_START = \"2025-06-01\"\n",
        "DATE_END   = \"2025-09-30\"  # extend if you have newer data\n",
        "UNIT_QUERY = \"\"            # e.g., \"236th Artillery\", \"752nd\", leave \"\" for no unit filter\n",
        "BASEMAP    = \"Stamen Terrain\"  # choices: \"OpenStreetMap\", \"Stamen Terrain\", \"OpenTopoMap\", \"Esri Satellite\"\n",
        "USE_CLASS_COLORS = True    # color markers by platform_class instead of all red\n",
        "# -----------------------------------------\n",
        "\n",
        "# Guards\n",
        "assert \"df\" in globals(), \"df not loaded. Run A1 first.\"\n",
        "assert \"paths\" in globals() and \"plots\" in paths, \"paths['plots'] missing. Run A1 first.\"\n",
        "RUN_LABEL = globals().get(\"RUN_LABEL\", \"latest\")\n",
        "os.makedirs(paths[\"plots\"], exist_ok=True)\n",
        "\n",
        "# Basemap registry\n",
        "BASEMAPS = {\n",
        "    \"OpenStreetMap\": dict(tiles=\"OpenStreetMap\", attr=None),\n",
        "    \"Stamen Terrain\": dict(tiles=\"Stamen Terrain\", attr=None),\n",
        "    \"OpenTopoMap\": dict(\n",
        "        tiles=\"https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png\",\n",
        "        attr=\"Map data: © OpenStreetMap contributors, SRTM | Map style: © OpenTopoMap (CC-BY-SA)\"\n",
        "    ),\n",
        "    \"Esri Satellite\": dict(\n",
        "        tiles=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
        "        attr=\"Tiles © Esri\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "if BASEMAP not in BASEMAPS:\n",
        "    raise ValueError(f\"Unknown BASEMAP '{BASEMAP}'. Choose one of: {list(BASEMAPS)}\")\n",
        "\n",
        "# Prepare data with unit fallback\n",
        "d = df.copy()\n",
        "d[\"date\"] = pd.to_datetime(d[\"date\"], errors=\"coerce\")\n",
        "\n",
        "unit_series = d[\"unit_canonical\"] if \"unit_canonical\" in d.columns else pd.Series([None]*len(d))\n",
        "if (unit_series.isna().all() or (unit_series.astype(str).str.strip() == \"\").all()) and \"unit_text\" in d.columns:\n",
        "    unit_series = d[\"unit_text\"]\n",
        "d[\"unit\"] = unit_series.astype(\"string\").str.strip()\n",
        "d.loc[d[\"unit\"].isin([\"\", \"None\", \"nan\"]), \"unit\"] = pd.NA\n",
        "\n",
        "# Filters\n",
        "mask = d[\"date\"].between(DATE_START, DATE_END) & d[\"lat\"].notna() & d[\"lon\"].notna()\n",
        "if DEFAULT_FOCUS_CLASSES:\n",
        "    mask &= d[\"platform_class\"].isin(DEFAULT_FOCUS_CLASSES)\n",
        "if UNIT_QUERY:\n",
        "    q = UNIT_QUERY.strip().lower()\n",
        "    mask &= d[\"unit\"].fillna(\"\").str.lower().str.contains(q)\n",
        "\n",
        "dm = d[mask].copy()\n",
        "\n",
        "if dm.empty:\n",
        "    print(\"[U1+] No rows match current filters. Try loosening UNIT_QUERY, classes, or dates.\")\n",
        "else:\n",
        "    # Class color palette (simple & readable)\n",
        "    class_colors = {\n",
        "        \"MBT\": \"#e41a1c\",   # red\n",
        "        \"IFV\": \"#377eb8\",   # blue\n",
        "        \"APC\": \"#4daf4a\",   # green\n",
        "        \"BMP\": \"#984ea3\",   # purple\n",
        "        \"SPG\": \"#ff7f00\",   # orange\n",
        "        \"MLRS\":\"#a65628\",   # brown\n",
        "        \"Truck\":\"#999999\",  # gray\n",
        "    }\n",
        "    def marker_color(cls):\n",
        "        if not USE_CLASS_COLORS:\n",
        "            return \"red\"\n",
        "        return class_colors.get(cls, \"#555555\")\n",
        "\n",
        "    # Center on mean of points\n",
        "    center_lat = dm[\"lat\"].mean()\n",
        "    center_lon = dm[\"lon\"].mean()\n",
        "\n",
        "    # Build map\n",
        "    bm = BASEMAPS[BASEMAP]\n",
        "    m = folium.Map(location=[center_lat, center_lon], zoom_start=6,\n",
        "                   tiles=bm[\"tiles\"], attr=bm[\"attr\"])\n",
        "\n",
        "    # Add a layer control and (optional) also include OSM for quick switching\n",
        "    if BASEMAP != \"OpenStreetMap\":\n",
        "        folium.TileLayer(\"OpenStreetMap\", name=\"OpenStreetMap\").add_to(m)\n",
        "    if BASEMAP != \"Stamen Terrain\":\n",
        "        folium.TileLayer(\"Stamen Terrain\", name=\"Stamen Terrain\").add_to(m)\n",
        "    if BASEMAP != \"OpenTopoMap\":\n",
        "        folium.TileLayer(\n",
        "            tiles=BASEMAPS[\"OpenTopoMap\"][\"tiles\"],\n",
        "            attr=BASEMAPS[\"OpenTopoMap\"][\"attr\"],\n",
        "            name=\"OpenTopoMap\"\n",
        "        ).add_to(m)\n",
        "    if BASEMAP != \"Esri Satellite\":\n",
        "        folium.TileLayer(\n",
        "            tiles=BASEMAPS[\"Esri Satellite\"][\"tiles\"],\n",
        "            attr=BASEMAPS[\"Esri Satellite\"][\"attr\"],\n",
        "            name=\"Esri Satellite\"\n",
        "        ).add_to(m)\n",
        "\n",
        "    cluster = MarkerCluster(name=\"Losses\").add_to(m)\n",
        "\n",
        "    # Add one marker per loss\n",
        "    for _, r in dm.sort_values(\"date\").iterrows():\n",
        "        unit_display = r.get(\"unit\") or \"Unknown unit\"\n",
        "        label = f\"\"\"\n",
        "        <b>{r.get('model','Unknown')}</b><br>\n",
        "        {r.get('platform_class','')}<br>\n",
        "        <i>{unit_display}</i><br>\n",
        "        <b>{r['date'].date()}</b><br>\n",
        "        <small>{r.get('location','')}</small>\n",
        "        \"\"\"\n",
        "        folium.CircleMarker(\n",
        "            location=(r[\"lat\"], r[\"lon\"]),\n",
        "            radius=4,\n",
        "            color=marker_color(r.get(\"platform_class\")),\n",
        "            fill=True,\n",
        "            fill_opacity=0.75,\n",
        "            tooltip=folium.Tooltip(label, sticky=True),\n",
        "        ).add_to(cluster)\n",
        "\n",
        "    folium.LayerControl(collapsed=True).add_to(m)\n",
        "\n",
        "    # Output file name with a tidy unit slug\n",
        "    def slugify(s):\n",
        "        if not s:\n",
        "            return \"all-units\"\n",
        "        s = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", s.strip())\n",
        "        return re.sub(r\"-+\", \"-\", s).strip(\"-\").lower()\n",
        "\n",
        "    unit_slug = slugify(UNIT_QUERY) if UNIT_QUERY else \"all-units\"\n",
        "    out_html = os.path.join(paths[\"plots\"], f\"map_per_loss_{unit_slug}_{BASEMAP.replace(' ','_')}_{RUN_LABEL}.html\")\n",
        "    m.save(out_html)\n",
        "\n",
        "    print(f\"✅ Saved interactive map: {out_html}\")\n",
        "    print(f\"Filters → unit: '{UNIT_QUERY or 'ALL'}', classes: {sorted(DEFAULT_FOCUS_CLASSES) if DEFAULT_FOCUS_CLASSES else 'ALL'}, \"\n",
        "          f\"dates: {DATE_START} → {DATE_END}, basemap: {BASEMAP}\")\n"
      ],
      "metadata": {
        "id": "QfR8EIJlDhhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}