{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f431a7",
   "metadata": {},
   "source": [
    "\n",
    "# ISW ORBAT ETL + WarSpotting Enrichment (MVP)\n",
    "\n",
    "**Purpose:** Parse the ISW Russian ORBAT PDF (2023-10-12), extract a clean unit dataset and hierarchy,\n",
    "build a robust alias bank, and enrich WarSpotting loss rows with ISW context (unit UID, echelon, service, military district).\n",
    "\n",
    "**Key design choices (per user requirements):**\n",
    "- **No garrison** fields (ignore basing locations)\n",
    "- **No confidence scores** (we’ll skip bracket status for MVP)\n",
    "- **Deterministic schema** and **linkable** to existing WarSpotting CSVs\n",
    "\n",
    "**Outputs:**\n",
    "- `isw_units.csv` — one row per unit\n",
    "- `isw_hierarchy.csv` — parent→child edges\n",
    "- `isw_aliases.csv` — aliases for matching\n",
    "- `warspotting_enriched.csv` — original WarSpotting rows + ISW enrichment columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Colab or a fresh environment, uncomment the following installs:\n",
    "# %pip install pdfminer.six PyPDF2 rapidfuzz Unidecode\n",
    "#\n",
    "# pdfminer.six  -> robust text extraction from PDFs\n",
    "# PyPDF2        -> secondary extractor (fallback)\n",
    "# rapidfuzz     -> fast, reliable fuzzy string matching\n",
    "# Unidecode     -> simple transliteration for aliases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import hashlib\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz, process  # type: ignore\n",
    "except Exception:\n",
    "    fuzz = None\n",
    "    process = None\n",
    "\n",
    "try:\n",
    "    from unidecode import unidecode  # type: ignore\n",
    "except Exception:\n",
    "    def unidecode(s: str) -> str:\n",
    "        return s\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = Path(\"/mnt/data\")\n",
    "ISW_PDF = DATA_DIR / \"October 12, 2023 Russian Orbat_Final.pdf\"\n",
    "WARSPOTTING_CSV = DATA_DIR / \"warspotting_norm_2025-09-08 (1).csv\"\n",
    "OUT_DIR = DATA_DIR / \"isw_orbat_etl\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "UNITS_CSV = OUT_DIR / \"isw_units.csv\"\n",
    "HIER_CSV  = OUT_DIR / \"isw_hierarchy.csv\"\n",
    "ALIAS_CSV = OUT_DIR / \"isw_aliases.csv\"\n",
    "ENRICHED_CSV = OUT_DIR / \"warspotting_enriched.csv\"\n",
    "\n",
    "SOURCE_VERSION = \"2023-10-12_ISW_ORBAT\"\n",
    "AS_OF = \"2023-01-01\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4bac51",
   "metadata": {},
   "source": [
    "## PDF Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff64c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_pdf(path: Path) -> str:\n",
    "    text = \"\"\n",
    "    try:\n",
    "        from pdfminer.high_level import extract_text as pdfminer_extract_text  # type: ignore\n",
    "        text = pdfminer_extract_text(str(path))\n",
    "        if text and len(text.strip()) > 0:\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(\"[warn] pdfminer.six failed:\", e)\n",
    "\n",
    "    try:\n",
    "        import PyPDF2  # type: ignore\n",
    "        with open(path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(\"[warn] PyPDF2 failed:\", e)\n",
    "\n",
    "    return text\n",
    "\n",
    "if not ISW_PDF.exists():\n",
    "    print(f\"[warn] ISW PDF not found at: {ISW_PDF}\")\n",
    "else:\n",
    "    print(f\"[ok] ISW PDF found at: {ISW_PDF}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a2e80",
   "metadata": {},
   "source": [
    "## Parsing Spec & Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c22fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ECHELON_PATTERNS = [\n",
    "    (\"army_corps\", r\"\\bArmy Corps\\b|\\bAC\\b\"),\n",
    "    (\"army\",       r\"\\bCombined Arms Army\\b|\\bCAA\\b|\\bArmy\\b\"),\n",
    "    (\"division\",   r\"\\bDivision\\b|\\bDiv\\.\\b\"),\n",
    "    (\"brigade\",    r\"\\bBrigade\\b|\\bBde\\b\"),\n",
    "    (\"regiment\",   r\"\\bRegiment\\b|\\bReg\\.\\b\"),\n",
    "    (\"battalion\",  r\"\\bBattalion\\b|\\bBn\\b|\\bBttn\\b\"),\n",
    "]\n",
    "\n",
    "SERVICE_HEADERS = {\n",
    "    \"army\": [\"Ground Forces\", \"Combined Arms Army\", \"Army\"],\n",
    "    \"navy_coastal\": [\"Coastal Defense\", \"Naval Infantry\", \"Navy\"],\n",
    "    \"vdv\": [\"Airborne Forces\", \"VDV\", \"Air Assault\"],\n",
    "    \"gru_spetsnaz\": [\"Spetsnaz\", \"GRU\"]\n",
    "}\n",
    "\n",
    "MD_HEADERS = {\n",
    "    \"WMD\": [\"Western Military District\"],\n",
    "    \"SMD\": [\"Southern Military District\"],\n",
    "    \"CMD\": [\"Central Military District\"],\n",
    "    \"EMD\": [\"Eastern Military District\"],\n",
    "    \"NFM\": [\"Northern Fleet\"]\n",
    "}\n",
    "\n",
    "VCH_REGEX = re.compile(r\"(?:в/ч|v/ch|vch)\\s*[:#]?\\s*(\\d{5})\")\n",
    "\n",
    "IS_BRACKETED = re.compile(r\"\\[.*\\]\")\n",
    "\n",
    "def normalize_unit_text(s: str) -> str:\n",
    "    s = unidecode(s or \"\")\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^\\w\\s\\-\\/]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def extract_number_token(name: str) -> Optional[str]:\n",
    "    m = re.search(r\"\\b(\\d{1,4})(?:st|nd|rd|th)?\\b\", name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "ABBREV_MAP = {\n",
    "    \"motorized rifle regiment\": \"mrr\",\n",
    "    \"motor rifle regiment\": \"mrr\",\n",
    "    \"motorized rifle division\": \"mrd\",\n",
    "    \"motor rifle division\": \"mrd\",\n",
    "    \"tank regiment\": \"tr\",\n",
    "    \"tank division\": \"td\",\n",
    "    \"air assault regiment\": \"aar\",\n",
    "    \"airborne regiment\": \"abr\",\n",
    "    \"artillery regiment\": \"ar\",\n",
    "    \"motorized rifle brigade\": \"mrb\",\n",
    "    \"motor rifle brigade\": \"mrb\",\n",
    "    \"separate motorized rifle brigade\": \"smrb\",\n",
    "    \"separate tank battalion\": \"stb\",\n",
    "    \"air assault brigade\": \"aab\",\n",
    "    \"separate artillery brigade\": \"sab\",\n",
    "    \"brigade\": \"bde\",\n",
    "    \"division\": \"div\",\n",
    "    \"regiment\": \"reg\",\n",
    "    \"battalion\": \"bn\",\n",
    "    \"army corps\": \"ac\",\n",
    "    \"combined arms army\": \"caa\",\n",
    "    \"army\": \"army\",\n",
    "}\n",
    "\n",
    "RU_MAP = {\n",
    "    \"motorized rifle regiment\": \"мотострелковый полк\",\n",
    "    \"motorized rifle division\": \"мотострелковая дивизия\",\n",
    "    \"tank regiment\": \"танковый полк\",\n",
    "    \"tank division\": \"танковая дивизия\",\n",
    "    \"air assault regiment\": \"десантно-штурмовой полк\",\n",
    "    \"air assault brigade\": \"десантно-штурмовая бригада\",\n",
    "    \"regiment\": \"полк\",\n",
    "    \"division\": \"дивизия\",\n",
    "    \"brigade\": \"бригада\",\n",
    "    \"battalion\": \"батальон\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_echelon(line: str) -> Optional[str]:\n",
    "    for ekind, pattern in ECHELON_PATTERNS:\n",
    "        if re.search(pattern, line, flags=re.IGNORECASE):\n",
    "            return ekind\n",
    "    return None\n",
    "\n",
    "def generate_unit_uid(vch: Optional[str], name: str, path_hint: str) -> str:\n",
    "    if vch:\n",
    "        return f\"ISW23:VCH_{vch}\"\n",
    "    h = hashlib.sha1((name + \"|\" + path_hint).encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"ISW23:HASH_{h}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_aliases(unit_name_official: str):\n",
    "    aliases = []\n",
    "    official = unit_name_official.strip()\n",
    "    aliases.append((official, \"EN\", \"official\"))\n",
    "\n",
    "    norm = normalize_unit_text(official)\n",
    "    num = extract_number_token(norm)\n",
    "\n",
    "    t = None\n",
    "    for k in ABBREV_MAP.keys():\n",
    "        if k in norm:\n",
    "            t = k\n",
    "            break\n",
    "\n",
    "    if num and t:\n",
    "        abbrev = ABBREV_MAP.get(t, None)\n",
    "        if abbrev:\n",
    "            aliases.append((f\"{num} {abbrev}\".upper(), \"EN\", \"abbrev_num_space\"))\n",
    "            aliases.append((f\"{num}{abbrev}\".upper(), \"EN\", \"abbrev_num_concat\"))\n",
    "            aliases.append((f\"{num} {abbrev}\".lower(), \"EN\", \"abbrev_num_space\"))\n",
    "            aliases.append((f\"{num}{abbrev}\".lower(), \"EN\", \"abbrev_num_concat\"))\n",
    "            aliases.append((f\"{num}th {abbrev}\".lower(), \"EN\", \"abbrev_ordinal\"))\n",
    "\n",
    "    if num and t and t in RU_MAP:\n",
    "        ru = RU_MAP[t]\n",
    "        aliases.append((f\"{num} {ru}\", \"RU\", \"ru_native_num\"))\n",
    "        aliases.append((f\"{num}-й {ru}\", \"RU\", \"ru_native_num_ord\"))\n",
    "\n",
    "    aliases.append((norm, \"EN\", \"normalized\"))\n",
    "    return aliases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_isw_text(raw_text: str):\n",
    "    units = []\n",
    "    edges = []\n",
    "    aliases = []\n",
    "\n",
    "    current_service = None\n",
    "    current_md = None\n",
    "\n",
    "    lines = [ln.strip() for ln in raw_text.splitlines() if ln.strip()]\n",
    "    path_hint = []\n",
    "    last_unit_by_level = {\"army\":None, \"army_corps\":None, \"division\":None, \"brigade\":None, \"regiment\":None, \"battalion\":None}\n",
    "\n",
    "    def reset_below(level: str):\n",
    "        order = [\"army_corps\",\"army\",\"division\",\"brigade\",\"regiment\",\"battalion\"]\n",
    "        if level in order:\n",
    "            idx = order.index(level)\n",
    "            for l in order[idx+1:]:\n",
    "                last_unit_by_level[l] = None\n",
    "\n",
    "    for ln in lines:\n",
    "        for service, keys in SERVICE_HEADERS.items():\n",
    "            if any(k.lower() in ln.lower() for k in keys):\n",
    "                current_service = service\n",
    "                path_hint = [service]\n",
    "                break\n",
    "\n",
    "        for md, keys in MD_HEADERS.items():\n",
    "            if any(k.lower() in ln.lower() for k in keys):\n",
    "                current_md = md\n",
    "                path_hint = [*(path_hint or []), md]\n",
    "                break\n",
    "\n",
    "        echelon = detect_echelon(ln)\n",
    "        if not echelon:\n",
    "            continue\n",
    "\n",
    "        vch = None\n",
    "        m = VCH_REGEX.search(ln)\n",
    "        if m:\n",
    "            vch = m.group(1)\n",
    "\n",
    "        name_official = re.sub(VCH_REGEX, \"\", ln).strip(\" -–—:·•\")\n",
    "        name_official = re.sub(r\"^[\\u2022\\-\\*•]+\", \"\", name_official).strip()\n",
    "\n",
    "        uid = generate_unit_uid(vch, name_official, \" > \".join(path_hint))\n",
    "\n",
    "        unit_row = {\n",
    "            \"unit_uid\": uid,\n",
    "            \"unit_name_official\": name_official,\n",
    "            \"echelon\": echelon,\n",
    "            \"service\": current_service or \"\",\n",
    "            \"military_district\": current_md or \"\",\n",
    "            \"as_of\": AS_OF,\n",
    "            \"source_version\": SOURCE_VERSION,\n",
    "        }\n",
    "        units.append(unit_row)\n",
    "\n",
    "        hierarchy_order = [\"army_corps\",\"army\",\"division\",\"brigade\",\"regiment\",\"battalion\"]\n",
    "        e_idx = hierarchy_order.index(echelon)\n",
    "        parent_uid = None\n",
    "        for j in range(e_idx-1, -1, -1):\n",
    "            higher = hierarchy_order[j]\n",
    "            if last_unit_by_level.get(higher):\n",
    "                parent_uid = last_unit_by_level[higher]\n",
    "                break\n",
    "\n",
    "        last_unit_by_level[echelon] = uid\n",
    "        reset_below(echelon)\n",
    "\n",
    "        if parent_uid:\n",
    "            edges.append({\n",
    "                \"parent_uid\": parent_uid,\n",
    "                \"child_uid\": uid,\n",
    "                \"parent_level\": hierarchy_order[hierarchy_order.index(echelon)-1] if e_idx>0 else \"\",\n",
    "                \"child_level\": echelon,\n",
    "            })\n",
    "\n",
    "        for alias_text, lang, kind in generate_aliases(name_official):\n",
    "            aliases.append({\n",
    "                \"unit_uid\": uid,\n",
    "                \"alias_text\": alias_text,\n",
    "                \"lang\": lang,\n",
    "                \"kind\": kind,\n",
    "            })\n",
    "\n",
    "    return units, edges, aliases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ISW_UNITS_HEADER = [\n",
    "    \"unit_uid\",\"unit_name_official\",\"echelon\",\"service\",\"military_district\",\"as_of\",\"source_version\"\n",
    "]\n",
    "\n",
    "ISW_HIER_HEADER = [\n",
    "    \"parent_uid\",\"child_uid\",\"parent_level\",\"child_level\"\n",
    "]\n",
    "\n",
    "ISW_ALIAS_HEADER = [\n",
    "    \"unit_uid\",\"alias_text\",\"lang\",\"kind\"\n",
    "]\n",
    "\n",
    "def write_csv(path: Path, rows: List[Dict], header: List[str]):\n",
    "    import csv\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k,\"\") for k in header})\n",
    "\n",
    "print(\"[ok] Headers prepared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4effce",
   "metadata": {},
   "source": [
    "## Pilot Extraction (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if ISW_PDF.exists():\n",
    "    raw = extract_text_pdf(ISW_PDF)\n",
    "    if not raw.strip():\n",
    "        print(\"[warn] PDF text extraction returned empty text.\")\n",
    "    else:\n",
    "        units, edges, aliases = parse_isw_text(raw)\n",
    "        print(f\"[info] Parsed units: {len(units)} | edges: {len(edges)} | aliases: {len(aliases)}\")\n",
    "        write_csv(UNITS_CSV, units, ISW_UNITS_HEADER)\n",
    "        write_csv(HIER_CSV, edges, ISW_HIER_HEADER)\n",
    "        write_csv(ALIAS_CSV, aliases, ISW_ALIAS_HEADER)\n",
    "        print(f\"[ok] Wrote: {UNITS_CSV}\")\n",
    "        print(f\"[ok] Wrote: {HIER_CSV}\")\n",
    "        print(f\"[ok] Wrote: {ALIAS_CSV}\")\n",
    "else:\n",
    "    print(\"[skip] ISW PDF not present; load later.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d6986",
   "metadata": {},
   "source": [
    "## WarSpotting Load & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def read_warspotting(path: Path) -> 'pd.DataFrame':\n",
    "    df = pd.read_csv(path)\n",
    "    for col in [\"unit_canonical\",\"unit_text\",\"model\",\"platform_class\",\"location\",\"tags\",\"status\",\"source\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"\")\n",
    "    return df\n",
    "\n",
    "def extract_vch_from_text(s: str) -> Optional[str]:\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    m = VCH_REGEX.search(s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def norm_for_matching(s: str) -> str:\n",
    "    s = unidecode(str(s or \"\"))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "if WARSPOTTING_CSV.exists():\n",
    "    ws_df = read_warspotting(WARSPOTTING_CSV)\n",
    "    print(\"[ok] WarSpotting loaded:\", len(ws_df))\n",
    "else:\n",
    "    ws_df = None\n",
    "    print(\"[warn] WarSpotting CSV not found; set the correct path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f933b82",
   "metadata": {},
   "source": [
    "## Matching & Enrichment (ISW → WarSpotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_isw_tables():\n",
    "    import pandas as pd\n",
    "    u = pd.read_csv(UNITS_CSV) if UNITS_CSV.exists() else pd.DataFrame(columns=ISW_UNITS_HEADER)\n",
    "    h = pd.read_csv(HIER_CSV)  if HIER_CSV.exists()  else pd.DataFrame(columns=ISW_HIER_HEADER)\n",
    "    a = pd.read_csv(ALIAS_CSV) if ALIAS_CSV.exists() else pd.DataFrame(columns=ISW_ALIAS_HEADER)\n",
    "    return u, h, a\n",
    "\n",
    "def build_alias_bank(alias_df):\n",
    "    bank = {}\n",
    "    for uid, sub in alias_df.groupby(\"unit_uid\"):\n",
    "        vals = []\n",
    "        for s in sub[\"alias_text\"].astype(str).tolist():\n",
    "            vals.append(s)\n",
    "            vals.append(norm_for_matching(s))\n",
    "        bank[uid] = list(dict.fromkeys(vals))\n",
    "    return bank\n",
    "\n",
    "def invert_alias_bank(bank):\n",
    "    inv = {}\n",
    "    for uid, aliases in bank.items():\n",
    "        for a in aliases:\n",
    "            na = norm_for_matching(a)\n",
    "            inv.setdefault(na, []).append(uid)\n",
    "    return inv\n",
    "\n",
    "def enrich_warspotting(ws, units_df, alias_df):\n",
    "    ws = ws.copy()\n",
    "    for col in [\"isw_unit_uid\",\"isw_unit_name_official\",\"isw_echelon\",\"isw_service\",\"isw_military_district\"]:\n",
    "        ws[col] = \"\"\n",
    "\n",
    "    bank = build_alias_bank(alias_df)\n",
    "    inv = invert_alias_bank(bank)\n",
    "    units_map = units_df.set_index(\"unit_uid\").to_dict(orient=\"index\")\n",
    "\n",
    "    def try_match(row):\n",
    "        text_combo = \" \".join([str(row.get(\"unit_canonical\",\"\")), str(row.get(\"unit_text\",\"\"))])\n",
    "        vch = extract_vch_from_text(text_combo)\n",
    "        if vch:\n",
    "            uid = f\"ISW23:VCH_{vch}\"\n",
    "            if uid in units_map:\n",
    "                return uid\n",
    "\n",
    "        for col in [\"unit_canonical\",\"unit_text\"]:\n",
    "            cand = norm_for_matching(row.get(col, \"\"))\n",
    "            if cand and cand in inv:\n",
    "                cands = inv[cand]\n",
    "                if len(cands) == 1:\n",
    "                    return cands[0]\n",
    "                lead_num = extract_number_token(cand)\n",
    "                if lead_num:\n",
    "                    for cu in cands:\n",
    "                        off = units_map[cu][\"unit_name_official\"]\n",
    "                        if lead_num in (extract_number_token(off or \"\") or \"\"):\n",
    "                            return cu\n",
    "                return cands[0]\n",
    "\n",
    "        if process is not None:\n",
    "            # Build alias universe once\n",
    "            alias_universe = []\n",
    "            rev_map = {}\n",
    "            for uid, alist in bank.items():\n",
    "                for a in alist:\n",
    "                    na = norm_for_matching(a)\n",
    "                    if na not in rev_map:\n",
    "                        rev_map[na] = uid\n",
    "                        alias_universe.append(na)\n",
    "\n",
    "            for col in [\"unit_canonical\",\"unit_text\"]:\n",
    "                q = str(row.get(col,\"\"))\n",
    "                if not q.strip():\n",
    "                    continue\n",
    "                from rapidfuzz import process, fuzz  # safe here\n",
    "                matches = process.extract(norm_for_matching(q), alias_universe, scorer=fuzz.WRatio, limit=5)\n",
    "                for alias_text, score, _idx in matches:\n",
    "                    if score >= 92:\n",
    "                        return rev_map[alias_text]\n",
    "\n",
    "        return None\n",
    "\n",
    "    for idx in range(len(ws)):\n",
    "        uid = try_match(ws.iloc[idx])\n",
    "        if uid:\n",
    "            ws.at[idx, \"isw_unit_uid\"] = uid\n",
    "            urec = units_map.get(uid, {})\n",
    "            ws.at[idx, \"isw_unit_name_official\"] = urec.get(\"unit_name_official\",\"\")\n",
    "            ws.at[idx, \"isw_echelon\"] = urec.get(\"echelon\",\"\")\n",
    "            ws.at[idx, \"isw_service\"] = urec.get(\"service\",\"\")\n",
    "            ws.at[idx, \"isw_military_district\"] = urec.get(\"military_district\",\"\")\n",
    "    return ws\n",
    "\n",
    "# Run enrichment if inputs exist (you can re-run after building ISW tables)\n",
    "if WARSPOTTING_CSV.exists() and UNITS_CSV.exists() and ALIAS_CSV.exists():\n",
    "    units_df, hier_df, alias_df = load_isw_tables()\n",
    "    enriched = enrich_warspotting(ws_df, units_df, alias_df)\n",
    "    enriched.to_csv(ENRICHED_CSV, index=False)\n",
    "    print(f\"[ok] Enriched CSV written: {ENRICHED_CSV} (rows={len(enriched)})\")\n",
    "else:\n",
    "    print(\"[info] Skipping enrichment run for now; ensure ISW tables and WarSpotting CSV exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460355a",
   "metadata": {},
   "source": [
    "## QA & Integrity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def qa_isw_tables(units_df, hier_df, alias_df):\n",
    "    print(\"Units:\", len(units_df), \"Edges:\", len(hier_df), \"Aliases:\", len(alias_df))\n",
    "    dup_uids = units_df[\"unit_uid\"][units_df[\"unit_uid\"].duplicated()].unique().tolist() if len(units_df)>0 else []\n",
    "    if dup_uids:\n",
    "        print(\"[warn] duplicate unit_uid:\", dup_uids[:5], \"…\")\n",
    "    if len(hier_df)>0:\n",
    "        missing = set(hier_df[\"child_uid\"]) - set(units_df[\"unit_uid\"])\n",
    "        if missing:\n",
    "            print(\"[warn] edges reference missing child uids:\", list(missing)[:5], \"…\")\n",
    "    if len(alias_df)>0:\n",
    "        alias_counts = alias_df.groupby(\"unit_uid\").size().describe()\n",
    "        print(\"Alias coverage (per unit):\\n\", alias_counts)\n",
    "\n",
    "# Example:\n",
    "# import pandas as pd\n",
    "# u, h, a = load_isw_tables()\n",
    "# qa_isw_tables(u, h, a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9bac5",
   "metadata": {},
   "source": [
    "\n",
    "## Notes / Tuning\n",
    "\n",
    "- Adjust `ECHELON_PATTERNS`, `SERVICE_HEADERS`, `MD_HEADERS` to exactly match headings in the ISW PDF once you view `extract_text_pdf` output.\n",
    "- Expand `ABBREV_MAP`/`RU_MAP` as you encounter more unit types.\n",
    "- For speed, prebuild a single alias universe (we do this inside the fuzzy branch now).\n",
    "- For tighter matches, add parent-path hints (e.g., prefer matches under “20th CAA” when the text mentions it).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
